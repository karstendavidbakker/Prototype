{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Machine Learning algorithm \n",
    "\n",
    "## Load data\n",
    "\n",
    "First, load the SDK of the Data-centric design Hub to connect to your Thing\n",
    "\n",
    "Note: like in the Python script for data collection, you need an .env file with your thing id and token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from dcd.entities.thing import Thing\n",
    "\n",
    "# The thing ID and access token\n",
    "load_dotenv()\n",
    "THING_ID = os.environ['THING_ID']\n",
    "THING_TOKEN = os.environ['THING_TOKEN']\n",
    "\n",
    "my_thing = Thing(thing_id=THING_ID, token=THING_TOKEN)\n",
    "my_thing.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the start and end dates, defining when to look for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "START_DATE = \"2019-05-16 10:53:00\"\n",
    "END_DATE = \"2019-05-16 10:56:00\"\n",
    "\n",
    "from_ts = datetime.timestamp(datetime.strptime(START_DATE, DATE_FORMAT)) * 1000\n",
    "to_ts = datetime.timestamp(datetime.strptime(END_DATE, DATE_FORMAT)) * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSR_PROP_NAME = \"FSR\"\n",
    "CLASS_PROP_NAME = \"Sitting Posture\"\n",
    "\n",
    "fsr = my_thing.find_property_by_name(FSR_PROP_NAME)\n",
    "fsr.read(from_ts, to_ts)\n",
    "data = fsr.values\n",
    "\n",
    "sitting = my_thing.find_property_by_name(CLASS_PROP_NAME)\n",
    "sitting.read(from_ts, to_ts)\n",
    "label = sitting.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract classes from the CLASS property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for index, clazz in enumerate(sitting.classes):\n",
    "    print(index, \" => \", clazz['name'])\n",
    "    classes.append(clazz['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training data (60%), cross validation data (20%) and test data (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_label = []\n",
    "cv_data = []\n",
    "cv_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "leftover_data = []\n",
    "leftover_label = []\n",
    "\n",
    "for index in range(len(data)):\n",
    "    # remove time\n",
    "    data[index].pop(0)\n",
    "    label[index].pop(0)\n",
    "    if index%5 == 0:\n",
    "        # 20% to test data\n",
    "        test_data.append(data[index])\n",
    "        test_label.append(label[index])\n",
    "    else:\n",
    "        # 80% leftover data\n",
    "        leftover_data.append(data[index])\n",
    "        leftover_label.append(label[index])\n",
    "\n",
    "for index in range(len(leftover_data)):\n",
    "    if index%4 == 0:\n",
    "        # 20% to cross validate\n",
    "        cv_data.append(leftover_data[index])\n",
    "        cv_label.append(leftover_label[index])\n",
    "    else:\n",
    "        # 60% to train\n",
    "        train_data.append(leftover_data[index])\n",
    "        train_label.append(leftover_label[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nb total data: \" + str(len(data)))\n",
    "print(\"nb total labels: \" + str(len(label)))\n",
    "\n",
    "print(\"nb train data: \" + str(len(train_data)))\n",
    "print(\"nb train labels: \" + str(len(train_label)))\n",
    "\n",
    "print(\"nb cv data: \" + str(len(cv_data)))\n",
    "print(\"nb cv labels: \" + str(len(cv_label)))\n",
    "\n",
    "print(\"nb test data: \" + str(len(test_data)))\n",
    "print(\"nb test labels: \" + str(len(test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "We use a k-Nearest Neighbour (kNN) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "Import evaluation functions from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the test data to evaluate the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = neigh.predict(cv_data)\n",
    "cvLabel = numpy.array(cv_label)\n",
    "result = accuracy_score(cvLabel, predicted)\n",
    "print(\"cv accuracy: {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation accuracy > 80%\n",
    "\n",
    "The validation passed, we can display the test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = neigh.predict(test_data)\n",
    "testLabel = numpy.array(test_label)\n",
    "testResult = accuracy_score(testLabel, predicted)\n",
    "print(\"test accuracy: {}\".format(testResult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(testLabel, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(testLabel, predicted, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(testLabel, predicted, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(testLabel, predicted, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(testLabel, predicted, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can show the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(testLabel, predicted, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation accuracy < 80%\n",
    "\n",
    "The validation failed, we can display the validation performance.\n",
    "\n",
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(cvLabel, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(cvLabel, predicted, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(cvLabel, predicted, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(cvLabel, predicted, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(cvLabel, predicted, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model to\n",
    "MODEL_FILE_NAME = \"model.pickle\"\n",
    "\n",
    "# import the pickle library\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "with io.open(MODEL_FILE_NAME, \"wb\") as file:\n",
    "    pickle.dump(neigh, file, protocol=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
